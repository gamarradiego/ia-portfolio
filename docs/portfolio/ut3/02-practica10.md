---
title: "Pr√°ctica 10: Data Augmentation Avanzado & Explicabilidad"
date: 2025-10-14
---

# Pr√°ctica 10 ‚Äì Data Augmentation Avanzado & Explicabilidad

---

## Contexto
En esta pr√°ctica se trabaj√≥ con el dataset **Oxford Flowers 102**, que contiene im√°genes en alta resoluci√≥n de 102 especies diferentes de flores.  
El objetivo principal fue construir un modelo de **clasificaci√≥n de im√°genes con Transfer Learning** e integrar t√©cnicas de **data augmentation**, **GradCAM** e **Integrated Gradients** para mejorar y explicar las predicciones del modelo.

---

## Objetivos

- Entrenar un modelo de clasificaci√≥n de flores con **EfficientNetB0** usando **transfer learning**.  
- Aplicar **data augmentation** para aumentar la robustez y capacidad de generalizaci√≥n.  
- Implementar **GradCAM** e **Integrated Gradients** para interpretar visualmente las predicciones del modelo.  
- Analizar el comportamiento del modelo frente a errores y explicar sus decisiones.  

---

## Actividades

| Actividad | Tiempo | Resultado esperado |
|------------|---------|--------------------|
| Instalaci√≥n de dependencias y carga del dataset | 15 min | Entorno configurado y dataset Oxford Flowers102 descargado y accesible. |
| Preparaci√≥n del dataset y creaci√≥n de pipelines (baseline y augmented) | 25 min | Dataset preprocesado con resizing, batching y normalizaci√≥n (EfficientNet). |
| Implementaci√≥n de Data Augmentation | 20 min | Pipeline con transformaciones geom√©tricas y fotom√©tricas activas. |
| Entrenamiento del modelo con EfficientNetB0 | 30 min | Modelo entrenado con accuracy superior al baseline sin augmentations. |
| Implementaci√≥n y an√°lisis de GradCAM | 30 min | Visualizaciones de activaciones para explicar las predicciones del modelo. |
| Implementaci√≥n y an√°lisis de Integrated Gradients | 25 min | Mapas de atribuci√≥n que confirmen o complementen la informaci√≥n de GradCAM. |
| An√°lisis de resultados y reflexi√≥n final | 20 min | Evaluaci√≥n de efectos del augmentation e interpretabilidad del modelo. |

---

## Desarrollo

### 1. Preparaci√≥n del entorno
Se instalaron las dependencias principales:  
```python hl_lines="2 6" linenums="1"
!pip install -q tensorflow tensorflow-datasets albumentations
```

Luego se importaron las librer√≠as necesarias (`tensorflow`, `tensorflow_datasets`, `albumentations`, etc.) y se configur√≥ el entorno de GPU para acelerar el entrenamiento.  

üìä *Confirmaci√≥n del entorno de GPU*   
![Setup](./img/p10_setup.png){ width="600" }
---

### 2. Carga y preprocesamiento del dataset
Se utiliz√≥ **Oxford Flowers 102**, cargado desde *TensorFlow Datasets (TFDS)*.  
El dataset incluye m√°s de 8.000 im√°genes en alta resoluci√≥n clasificadas en 102 categor√≠as.

üìå *Carga del dataset*
```python hl_lines="2 6" linenums="1"
(ds_train, ds_test), ds_info = tfds.load(
    'oxford_flowers102',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)
```

Para acelerar la pr√°ctica, se utiliz√≥ un subconjunto de 5.000 im√°genes de entrenamiento y 1.000 de prueba.

---

### 3. Creaci√≥n de pipelines de datos

Se implementaron dos pipelines:

- **Baseline:** con normalizaci√≥n mediante `preprocess_input` de EfficientNet.  
- **Augmented:** con transformaciones avanzadas (flip, rotaci√≥n, zoom, traslaci√≥n, brillo, contraste).

üìå *Definici√≥n del papeline base*
```python hl_lines="2 6" linenums="1"
def create_baseline_pipeline(dataset, batch_size=32, training=True):
    if training:
        dataset = dataset.shuffle(1000)

    dataset = dataset.batch(batch_size)

    # Aplicar normalizaci√≥n DESPU√âS de batching
    def normalize_batch(images, labels):
        # Normalizar con EfficientNet preprocessing
        images = preprocess_input(images)
        return images, labels

    dataset = dataset.map(normalize_batch, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    return dataset
```

üìå *Definici√≥n de la capa de augmentation*
```python hl_lines="2 6" linenums="1"
    def augment_layer():
    """
    Crea capa de augmentation con Keras
    """
    return keras.Sequential([
        # Geom√©trico
        layers.RandomFlip("horizontal"),  # horizontal o vertical
        layers.RandomRotation(0.125),  # factor de rotaci√≥n (0.125 = 45¬∞)
        layers.RandomZoom(0.1),
        layers.RandomTranslation(0.1, 0.1),

        # Fotom√©trico
        layers.RandomBrightness(0.2),
        layers.RandomContrast(0.2),

    ], name="augmentation")
```

Primero se realizan cuatro transformaciones geom√©tricas:
    
**RandomFlip("horizontal"):**
    
   Invierte aleatoriamente las im√°genes en el eje horizontal (de izquierda a derecha).  
   Permite que el modelo aprenda que la orientaci√≥n lateral de una flor no cambia su clase.  
   Ejemplo: una flor mirando a la izquierda o a la derecha debe considerarse la misma categor√≠a.

**RandomRotation(0.125):**

   Rota la imagen aleatoriamente hasta ¬±45¬∞.  
   Simula variaciones en el √°ngulo de la c√°mara o la posici√≥n de la flor.  
   Beneficio: el modelo se vuelve m√°s invariante a rotaciones moderadas.

**RandomZoom(0.1):**

   Acerca o aleja la imagen hasta un 10%.  
   Ayuda a manejar variaciones de distancia entre la c√°mara y el objeto.

**RandomTranslation(0.1, 0.1):**

   Desplaza aleatoriamente la imagen hasta un 10% del ancho y alto.  
   Emula peque√±os desplazamientos de c√°mara o recortes en la toma. 
   
Luego, se realizan dos transformaciones geom√©tricas:

**RandomBrightness(0.2):**

   Ajusta el brillo de la imagen de forma aleatoria ¬±20%.

**RandomContrast(0.2):**

   Modifica el contraste de la imagen ¬±20%.
    


üìä *Visualizaci√≥n del augmentation*   
![Visualizaci√≥n de augmentation](./img/p10_visualizacion_augmentation.png){ width="600" }

Estas transformaciones ayudan a simular condiciones reales (cambios de iluminaci√≥n, orientaci√≥n, escala) y mejorar la generalizaci√≥n del modelo.

---

### 4. Creaci√≥n y entrenamiento del modelo

El modelo base seleccionado fue **EfficientNetB0**, preentrenado en ImageNet.  
Se us√≥ `include_top=False` para reemplazar la cabeza de clasificaci√≥n y adaptarlo a las 102 clases de flores.
  
üìå *Definici√≥n del modelo usando EfficientNetB0*
```python hl_lines="2 6" linenums="1"
    def create_model():
        base_model = keras.applications.EfficientNetB0( 
            include_top=False,
            weights='imagenet',
            input_shape=(IMG_SIZE, IMG_SIZE, 3)
        )
        base_model.trainable = False

        model = keras.Sequential([
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dropout(0.2),
            layers.Dense(NUM_CLASSES, activation='softmax')
        ])

        return model
```

El modelo fue compilado con:
```python hl_lines="2 6" linenums="1"
    optimizer='adam'
    loss='sparse_categorical_crossentropy'
    metrics=['accuracy']
```

Se entren√≥ por 8 √©pocas con data augmentation activo y validaci√≥n sobre el conjunto de test.

üìà *Gr√°ficos de loss y accuracy*   
![Gr√°ficos de loss y accuracy](./img/p10_accuracy_loss.png){ width="600" }

---

### 5. GradCAM ‚Äì Interpretaci√≥n visual de predicciones

Se implement√≥ **GradCAM** para visualizar las regiones de la imagen que el modelo considera m√°s relevantes al predecir. 

üìå *Implementaci√≥n del algoritmo GradCAM*
```python hl_lines="2 6" linenums="1"
    def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
        if hasattr(model.layers[0], 'layers'):
            # Modelo Sequential con base model adentro
            base_model = model.layers[0]
            conv_layer = base_model.get_layer(last_conv_layer_name)

            grad_model = keras.Model(
                inputs=base_model.input,
                outputs=[conv_layer.output, base_model.output]
            )

            # Calcular con el modelo completo
            with tf.GradientTape() as tape:
                # Forward pass por el modelo base
                conv_outputs, base_output = grad_model(img_array)

                # Aplicar las capas del classifier (despu√©s del base model)
                classifier_input = base_output
                for layer in model.layers[1:]:  # Capas despu√©s del base model
                    classifier_input = layer(classifier_input)
                predictions = classifier_input

                if pred_index is None:
                    pred_index = tf.argmax(predictions[0])
                class_channel = predictions[:, pred_index]

            # Gradientes respecto a conv_outputs
            grads = tape.gradient(class_channel, conv_outputs)
        else:
            # Modelo simple (no anidado)
            conv_layer = model.get_layer(last_conv_layer_name)
            grad_model = keras.Model(
                inputs=model.inputs,
                outputs=[conv_layer.output, model.output]
            )

            with tf.GradientTape() as tape:
                conv_outputs, predictions = grad_model(img_array)
                if pred_index is None:
                    pred_index = tf.argmax(predictions[0])
                class_channel = predictions[:, pred_index]

            grads = tape.gradient(class_channel, conv_outputs)

        # Pooling de gradientes
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

        # Importancia de cada filtro
        conv_outputs = conv_outputs[0]
        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
        heatmap = tf.squeeze(heatmap)

        # Normalizar entre 0 y 1
        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
        return heatmap.numpy()
```

En la funci√≥n anterior se implementa el algoritmo GradCAM, una t√©cnica de explainable AI (XAI) que permite visualizar qu√© regiones de una imagen activan m√°s fuertemente el modelo para una clase determinada. A trav√©s de mapas de calor deja en evidencia las regiones m√°s importantes de una imagen.

üìå *Visualizaci√≥n del resultado de GradCAM*
```python hl_lines="2 6" linenums="1"
    def visualize_gradcam(image, heatmap, predicted_class, true_class):
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Desnormalizar imagen
        img_display = image[0].numpy().copy()
        img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())
        img_display = np.clip(img_display, 0, 1)

        # Resize heatmap al tama√±o de la imagen
        img_size = image.shape[1]  # 224
        heatmap_resized = np.array(Image.fromarray(heatmap).resize((img_size, img_size)))

        # 1. Original
        axes[0].imshow(img_display)
        axes[0].set_title("Imagen Original", fontsize=12)
        axes[0].axis('off')

        # 2. Heatmap
        axes[1].imshow(heatmap_resized, cmap='jet')
        axes[1].set_title("GradCAM Heatmap", fontsize=12)
        axes[1].axis('off')

        # 3. Overlay (combinado)
        axes[2].imshow(img_display)
        axes[2].imshow(heatmap_resized, cmap='jet', alpha=0.4)
        axes[2].set_title(f"Predicci√≥n: {predicted_class}\nReal: {true_class}", fontsize=12)
        axes[2].axis('off')

        plt.tight_layout()
        plt.show()
```

Con esta √∫ltima funci√≥n se visualiza el resultado de GradCAM de forma clara y comparativa.

üìä *Ejemplos del resultado de GradCAM*   
![GradCAM 1](./img/p10_gradcam1.png){ width="600" }
![GradCAM 2](./img/p10_gradcam2.png){ width="600" }
![GradCAM 3](./img/p10_gradcam3.png){ width="600" }
![GradCAM 4](./img/p10_gradcam4.png){ width="600" }
![GradCAM 5](./img/p10_gradcam5.png){ width="600" }

Esto permite verificar si el modelo ‚Äúmira‚Äù las partes correctas de las flores (p√©talos, centro, textura, etc.) al clasificar, lo cual aumenta la confianza en las predicciones del modelo desarrollado.
La idea es mostrar una selecci√≥n de resultados que incluya aciertos y algunos errores. Como se puede ver en la primera imagen, el modelo est√° mirando las partes correctas de la flor y la clasificaci√≥n es correcta. En el caso de la segunda imagen, la clasificaci√≥n es correcta, sin embargo el heatmap producido por GradCAM demuestra que el modelo no est√° mirando lo que deber√≠a, en particular en la flor que se encuentra en la parte inferior de la imagen. Lo mismo pasa con la tercera imagen, donde la flor superior parece bien identificada y sin embargo, la que se encuentra a la derecha (y es m√°s grande) parece no ser mirada por el modelo.
En las √∫ltimas dos fotos vemos ejemplos claros de problemas en la clasificaci√≥n y basta con ver el heatmap para observar que no est√° mirando lo que debe mirar.

---

### 6. Integrated Gradients ‚Äì Explicabilidad complementaria

Luego, se aplic√≥ la t√©cnica de **Integrated Gradients**, que calcula el aporte de cada p√≠xel a la decisi√≥n final.  
üìå *Funci√≥n para aplicar Integrated Gradients*
```python hl_lines="2 6" linenums="1"
    def apply_integrated_gradients(model, image, class_idx, baseline=None, steps=150):
        # Baseline (imagen negra por defecto)
        if baseline is None:
            baseline = tf.zeros_like(image)

        # Generar alphas para interpolaci√≥n
        alphas = tf.linspace(0.0, 1.0, steps)

        # Funci√≥n para calcular gradientes
        @tf.function
        def compute_gradients(images, target_class):
            with tf.GradientTape() as tape:
                tape.watch(images)
                predictions = model(images)
                loss = predictions[:, target_class]

            return tape.gradient(loss, images)

        # Interpolar entre baseline e imagen
        interpolated_images = baseline + alphas[:, tf.newaxis, tf.newaxis, tf.newaxis] * (image - baseline)

        # Calcular gradientes para cada interpolaci√≥n
        gradients = compute_gradients(interpolated_images, class_idx)

        # Aproximaci√≥n integral (promedio de gradientes)
        avg_gradients = tf.reduce_mean(gradients, axis=0)

        # Integrated gradients
        integrated_grads = (image - baseline) * avg_gradients

        return integrated_grads[0]
```

üìå *Funci√≥n para visualizar Integrated Gradients*
```python hl_lines="2 6" linenums="1"
    def visualize_integrated_gradients(image, attribution, predicted_class, true_class):
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Desnormalizar (mismo que GradCAM)
        img_display = image[0].numpy().copy()
        img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())
        img_display = np.clip(img_display, 0, 1)

        # Original
        axes[0].imshow(img_display)
        axes[0].set_title("Original Image")
        axes[0].axis('off')

        # Attribution
        attr_display = np.sum(np.abs(attribution.numpy()), axis=-1)
        attr_display = (attr_display - attr_display.min()) / (attr_display.max() - attr_display.min() + 1e-8)

        axes[1].imshow(attr_display, cmap='hot')
        axes[1].set_title("Integrated Gradients")
        axes[1].axis('off')

        # Overlay
        axes[2].imshow(img_display)
        axes[2].imshow(attr_display, cmap='hot', alpha=0.5)
        axes[2].set_title(f"Pred: {predicted_class}\nTrue: {true_class}")
        axes[2].axis('off')

        plt.tight_layout()
        plt.show()
```

Integrated Gradients cuantifica la contribuci√≥n de cada pixel al resultado del modelo. De esta forma, logra ofrecer una interpretaci√≥n m√°s estable y matem√°ticamente justificada que otros m√©todos basados en gradientes instantaneos (como podr√≠a serlo GradCAM).

üìä *Resultado de aplicar Integrated Gradients*   
![Integrated Gradients](./img/p10_integrated_gradients.png){ width="600" }
![Integrated Gradients](./img/p10_integrated_gradients2.png){ width="600" }

En la primera, se lo aplica sobre una imagen bien clasificada y donde GradCAM demostr√≥ que el modelo miraba correctamente. Integrated Gradients permite visualizar c√≥mo los pixeles del contorno de la flor, los de las hojas y sobretodo los del centro contribuyen fuertemente al resultado que dio el modelo.

---

## Evidencias
üìì **Notebook**
  
- [Archivo local del Notebook](./notebook/practica10.ipynb)  
- [Abrir en Google Colab](https://colab.research.google.com/drive/1jU-VKmmDeykvSvwLipp4SkHpDAbMRv5U?usp=sharing) 

---

## Reflexi√≥n

El uso de data augmentation no mejor√≥ la accuracy del modelo, que se mantuvo alrededor del 72-73%. Esto puede deberse a que el dataset de flores Oxford 102 ya tiene una gran variabilidad en colores, formas y texturas, por lo que las transformaciones aplicadas (rotaciones, traslaciones, cambios de brillo y contraste, etc.) no aportaron informaci√≥n realmente nueva. Quiz√°s, al contrario, falt√≥ diversidad en las transformaciones del data augmentation. No deja de ser una herramienta y proceso sumamente interesante ya que permite agregar muchisima variabilidad a los distintos dataset. En este caso, el aumento de datos no parece haber ayudado al modelo a generalizar mejor, sino que posiblemente introdujo ruido o variaciones poco relevantes para la tarea.

Al analizar 3-5 ejemplos de GradCAM, se observ√≥ que solo en uno de los aciertos el modelo estaba ‚Äúmirando‚Äù correctamente la regi√≥n principal de la flor. En los otros dos aciertos, el modelo acert√≥ en la clase pero enfocando su atenci√≥n en zonas irrelevantes del fondo o de los bordes. Esto podr√≠a sugerir que la predicci√≥n correcta podr√≠a deberse m√°s a una cuesti√≥n de coincidencia estad√≠stica que a una verdadera comprensi√≥n visual. Esto evidencia que un modelo puede tener buena accuracy, pero no necesariamente basar sus decisiones en caracter√≠sticas sem√°nticamente correctas.

En los casos donde el modelo se equivoc√≥, el GradCAM mostr√≥ que su atenci√≥n estaba desviada hacia el fondo o hacia elementos no florales (como hojas, tierra o iluminaci√≥n). Por ejemplo, en el segundo ejemplo presentado se ve que claramente la mayor parte de la atenci√≥n est√° en las hojas y el fondo. Esto explica por qu√© el modelo confundi√≥ ciertas clases: no aprendi√≥ a distinguir espec√≠ficamente los p√©talos o la estructura central de la flor, sino patrones globales de color o textura. Estos errores reflejan la necesidad de una mayor especializaci√≥n del modelo en las regiones realmente relevantes.

A la hora de implementar un sistema de identificaci√≥n de flores, explicar las predicciones del modelo es sumamente importante. La explicabilidad aporta confianza respecto a lo que la IA reconoce. Justamente los usuarios deben poder confiar en que el modelo identifica rasgos caracter√≠sticos o distintivos de cada especie y que no se basa en factores al azar.

A todo esto, dedicando m√°s tiempo y atenci√≥n al detalle, se podr√≠a implementar fine-tuning del modelo base, entrenar m√°s √©pocas, experimentar con otras arquitecturas (quiz√°s algunas m√°s profundas), ajustar el data augmentation como se menci√≥no anteriormente, incrementar el tama√±o del dataset.

---

## Referencias

- [TensorFlow / Keras API Documentation](https://www.tensorflow.org/api_docs/python/tf/keras)

- [Keras Applications ‚Äì EfficientNet y MobileNetV2](https://keras.io/api/applications/)  

- [Dataset Oxford Flowers 102 ‚Äì TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_flowers102)  

- [GradCAM: Visual Explanations from Deep Networks via Gradient-based Localization (Selvaraju et al., 2017)](https://arxiv.org/abs/1610.02391)  

- [Integrated Gradients (Sundararajan et al., 2017)](https://arxiv.org/abs/1703.01365)
